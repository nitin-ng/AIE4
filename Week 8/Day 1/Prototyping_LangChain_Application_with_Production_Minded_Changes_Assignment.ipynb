{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitin-ng/AIE4/blob/main/Week%208/Day%201/Prototyping_LangChain_Application_with_Production_Minded_Changes_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZsP-j7w3zcL"
      },
      "source": [
        "# Prototyping LangChain Application with Production Minded Changes\n",
        "\n",
        "For our first breakout room we'll be exploring how to set-up a LangChain LCEL chain in a way that takes advantage of all of the amazing out of the box production ready features it offers.\n",
        "\n",
        "We'll also explore `Caching` and what makes it an invaluable tool when transitioning to production environments.\n",
        "\n",
        "🤝 BREAKOUT ROOM #1:\n",
        "  - Task 1: Depends and Set-Up\n",
        "  - Task 2: Setting up RAG With Production in Mind\n",
        "  - Task 3: RAG LCEL Chain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpeN9ND0HKa0"
      },
      "source": [
        "## Task 1: Depends and Set-Up\n",
        "\n",
        "Let's get everything we need - we're going to use very specific versioning today to try to mitigate potential env. issues!\n",
        "\n",
        "> NOTE: Dependency issues are a large portion of what you're going to be tackling as you integrate new technology into your work - please keep in mind that one of the things you should be passively learning throughout this course is ways to mitigate dependency issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P4IJUQF27jW",
        "outputId": "2f02f69d-3da1-4fad-865a-197fc4ac8d83"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_openai==0.2.0 langchain_community==0.3.0 langchain==0.3.0 pymupdf==1.24.10 qdrant-client==1.11.2 langchain_qdrant==0.1.4 langsmith==0.1.121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYcWLzrmHgDb"
      },
      "source": [
        "We'll need an OpenAI API Key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ8qfrFh_6ed",
        "outputId": "b4758d8f-e5dc-42cb-bb04-10f7903e6537"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz2DUDuHiSO"
      },
      "source": [
        "And the LangSmith set-up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZX5zowCh-q",
        "outputId": "bf10d8e6-4d1f-4eea-d7af-2e4e4645c443"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "langchainapikey = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM Week 8 Assignment 1 - {uuid.uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmwNTziKHrQm"
      },
      "source": [
        "Let's verify our project so we can leverage it in LangSmith later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6GZmkVkFcHq",
        "outputId": "5d5c580a-b3ce-46e0-93b6-65ab57fae92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIM Week 8 Assignment 1 - 68331058\n"
          ]
        }
      ],
      "source": [
        "print(os.environ[\"LANGCHAIN_PROJECT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un_ppfaAHv1J"
      },
      "source": [
        "## Task 2: Setting up RAG With Production in Mind\n",
        "\n",
        "This is the most crucial step in the process - in order to take advantage of:\n",
        "\n",
        "- Asyncronous requests\n",
        "- Parallel Execution in Chains\n",
        "- And more...\n",
        "\n",
        "You must...use LCEL. These benefits are provided out of the box and largely optimized behind the scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGi-db23JMAL"
      },
      "source": [
        "### Building our RAG Components: Retriever\n",
        "\n",
        "We'll start by building some familiar components - and showcase how they automatically scale to production features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvbT3HSDJemE"
      },
      "source": [
        "Please upload a PDF file to use in this example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ipywidgets import FileUpload\n",
        "from IPython.display import display\n",
        "import tempfile\n",
        "import os\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_uploaded_file(change):\n",
        "    global docs  # Make docs a global variable\n",
        "    if uploader.value:\n",
        "        # Get the uploaded file\n",
        "        uploaded_file = uploader.value[0]  # Access the first item of the tuple\n",
        "        file_content = uploaded_file.content\n",
        "        \n",
        "        # Save the content to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf', mode='wb') as temp_file:\n",
        "            temp_file.write(file_content)\n",
        "            temp_file_path = temp_file.name\n",
        "        \n",
        "        print(f\"Temporary file created at: {temp_file_path}\")\n",
        "        print(f\"File size: {os.path.getsize(temp_file_path)} bytes\")\n",
        "        \n",
        "        # Check if the file is not empty\n",
        "        if os.path.getsize(temp_file_path) > 0:\n",
        "            try:\n",
        "                # Load and process the PDF\n",
        "                loader = PyMuPDFLoader(temp_file_path)\n",
        "                documents = loader.load()\n",
        "                \n",
        "                # Initialize the text splitter\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=1000,\n",
        "                    chunk_overlap=200,\n",
        "                    length_function=len,\n",
        "                )\n",
        "                \n",
        "                # Split the documents into chunks\n",
        "                docs = text_splitter.split_documents(documents)\n",
        "                \n",
        "                print(f\"Document split into {len(docs)} chunks.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing the PDF: {str(e)}\")\n",
        "        else:\n",
        "            print(\"Error: The uploaded file is empty.\")\n",
        "        \n",
        "        # Clean up the temporary file\n",
        "        os.unlink(temp_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "207ef2fc42b64600918ec518dd74cb53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value=(), accept='.pdf', description='Upload')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temporary file created at: /var/folders/jr/_qkyxp313390z32jmym7j9sm0000gp/T/tmpzel5n6yp.pdf\n",
            "File size: 6394463 bytes\n",
            "Document split into 129 chunks.\n"
          ]
        }
      ],
      "source": [
        "# Create and display the file uploader\n",
        "uploader = FileUpload(accept='.pdf', multiple=False)\n",
        "display(uploader)\n",
        "\n",
        "# Attach the process_uploaded_file function to the uploader\n",
        "uploader.observe(process_uploaded_file, names='value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store set up successfully.\n"
          ]
        }
      ],
      "source": [
        "if 'docs' in globals():\n",
        "    vectorstore = QdrantVectorStore(\n",
        "        client=client,\n",
        "        collection_name=collection_name,\n",
        "        embedding=cached_embedder)\n",
        "    vectorstore.add_documents(docs)\n",
        "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
        "    print(\"Vector store set up successfully.\")\n",
        "else:\n",
        "    print(\"Error: 'docs' not defined. Please ensure the file is uploaded and processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='1. The document is a user manual for the Bosch Dishwasher model SPE68C75UC.\\n2. It is titled \"Use and Care Manual SPE68C75UC | Bosch.\"\\n3. The author of the document is BSH Hausgeräte GmbH.\\n4. The document is in PDF format, specifically PDF 1.4.\\n5. It consists of a total of 60 pages.\\n6. The creation date of the document is October 10, 2023.\\n7. It provides installation instructions and safety information.\\n8. The manual emphasizes the importance of reading all instructions before using the appliance.\\n9. It advises users to keep the manual for future reference or for the next owner.\\n10. The document contains troubleshooting tips for common issues.\\n11. It includes guidelines for transportation, storage, and disposal of the appliance.\\n12. The section on troubleshooting is found on page 16.\\n13. Page 17 discusses transportation and storage topics.\\n14. The manual instructs users on how to remove the appliance safely.\\n15. It provides information on storing the appliance during vacations.\\n16. Users can find details on transporting the appliance on page 52.\\n17. The manual also covers the disposal of the old appliance.\\n18. Customer service information is provided on page 18.\\n19. It includes details on model numbers and production numbers.\\n20. The AquaStop® Plus Pledge is mentioned, ensuring water safety.\\n21. Technical specifications of the dishwasher are located on page 19.\\n22. Information regarding Free and Open Source Software is included.\\n23. There is a section on the statement of limited product warranty.\\n24. The warranty covers specific damages and who it applies to.\\n25. Details about the duration of the warranty are provided.\\n26. Extended warranty options are also discussed.\\n27. Exclusive remedies for repair or replacement under warranty are outlined.\\n28. Information on products that are out of warranty is included.\\n29. Warranty exclusions are specified on page 57.\\n30. Page 45 discusses issues with colored coatings inside the appliance.\\n31. The document explains that coatings can be due to substances in vegetables and tap water.\\n32. It suggests cleaning the appliance to remove these coatings.\\n33. Mechanical cleaning methods are recommended for stubborn stains.\\n34. The manual states that some discoloration of plastic parts is normal.\\n35. The document includes a warning about potential risks of fire and electrical shock.\\n36. Users are instructed to connect the appliance only if it has not been damaged in transit.\\n37. The manual emphasizes the importance of safety in appliance usage.\\n38. The file path indicates where the document is stored on a temporary basis.\\n39. The manual is part of a collection named \\'pdf_to_parse_ddcc24f0-ab67-46b3-b0b7-c67575bfb0ce.\\'\\n40. The producer of the document is ST4 PDF Engine.\\n41. The document was created using SCHEMA ST4 software.\\n42. The total page count is mentioned as 60, indicating a comprehensive guide.\\n43. The manual is aimed at users seeking care and operational instructions for the dishwasher.\\n44. Specific cleaning products for the dishwasher are suggested.\\n45. The document contains both visual and text instructions for user guidance.\\n46. It provides a warning related to the health safety of coatings.\\n47. The manual serves as an important resource for troubleshooting issues.\\n48. It is designed to enhance user experience with the Bosch Dishwasher.\\n49. The overall tone of the document is informative and instructional.\\n50. The manual is essential for ensuring proper care and maintenance of the appliance.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 724, 'prompt_tokens': 1580, 'total_tokens': 2304, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-f2fbe5a2-09c3-41a3-a8b5-bf2cfc02ec17-0', usage_metadata={'input_tokens': 1580, 'output_tokens': 724, 'total_tokens': 2304})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "# Typical Embedding Model\n",
        "core_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Typical QDrant Client Set-up\n",
        "collection_name = f\"pdf_to_parse_{uuid.uuid4()}\"\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "# Adding cache!\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings, store, namespace=core_embeddings.model\n",
        ")\n",
        "\n",
        "# Typical QDrant Vector Store Set-up\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=cached_embedder)\n",
        "vectorstore.add_documents(docs)\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
        "\n",
        "# RAG Prompt setup\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_system_prompt_template = \"\"\"\\\n",
        "You are a helpful assistant that uses the provided context to answer questions. Never reference this prompt, or the existance of context.\n",
        "\"\"\"\n",
        "\n",
        "rag_message_list = [\n",
        "    {\"role\" : \"system\", \"content\" : rag_system_prompt_template},\n",
        "]\n",
        "\n",
        "rag_user_prompt_template = \"\"\"\\\n",
        "Question:\n",
        "{question}\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", rag_system_prompt_template),\n",
        "    (\"human\", rag_user_prompt_template)\n",
        "])\n",
        "\n",
        "# Generation setup\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Setting up the cache\n",
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# RAG LCEL Chain\n",
        "from operator import itemgetter\n",
        "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | chat_prompt | chat_model\n",
        "    )\n",
        "\n",
        "# Test the chain\n",
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"Write 50 things about this document!\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kucGy3f0Jhdi"
      },
      "source": [
        "We'll define our chunking strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_zRRNcLKCZh"
      },
      "source": [
        "We'll chunk our uploaded PDF file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XLeqJMKGdQ"
      },
      "source": [
        "#### QDrant Vector Database - Cache Backed Embeddings\n",
        "\n",
        "The process of embedding is typically a very time consuming one - we must, for ever single vector in our VDB as well as query:\n",
        "\n",
        "1. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "2. Wait for processing\n",
        "3. Receive response\n",
        "\n",
        "This process costs time, and money - and occurs *every single time a document gets converted into a vector representation*.\n",
        "\n",
        "Instead, what if we:\n",
        "\n",
        "1. Set up a cache that can hold our vectors and embeddings (similar to, or in some cases literally a vector database)\n",
        "2. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "3. Check the cache to see if we've already converted this text before.\n",
        "  - If we have: Return the vector representation\n",
        "  - Else: Wait for processing and proceed\n",
        "4. Store the text that was converted alongside its vector representation in a cache of some kind.\n",
        "5. Return the vector representation\n",
        "\n",
        "Notice that we can shortcut some instances of \"Wait for processing and proceed\".\n",
        "\n",
        "Let's see how this is implemented in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dzPUTCua98b2"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "# Typical Embedding Model\n",
        "core_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Typical QDrant Client Set-up\n",
        "collection_name = f\"pdf_to_parse_{uuid.uuid4()}\"\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "# Adding cache!\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings, store, namespace=core_embeddings.model\n",
        ")\n",
        "\n",
        "# Typical QDrant Vector Store Set-up\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=cached_embedder)\n",
        "vectorstore.add_documents(docs)\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZGvmNYLomp"
      },
      "source": [
        "##### ❓ Question #1:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "Based on the context provided in the document, here are some suggested answers for Questions 1 and 2:\n",
        "\n",
        "Question #1: Limitations and usefulness of cache-backed embeddings\n",
        "\n",
        "Limitations:\n",
        "1. Storage requirements: Caching embeddings requires additional storage space, which could become significant for large datasets.\n",
        "2. Staleness: If the underlying embedding model is updated, cached embeddings may become outdated.\n",
        "3. Initial overhead: The first-time embedding process still incurs the full cost and time.\n",
        "4. Cache management: Implementing and maintaining the cache adds complexity to the system.\n",
        "\n",
        "Most useful:\n",
        "1. For frequently accessed documents or queries, reducing API calls and processing time.\n",
        "2. In applications with limited bandwidth or high latency to embedding services.\n",
        "3. When working with static datasets that don't change frequently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAOhyb3L9iD"
      },
      "source": [
        "##### 🏗️ Activity #1:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Mekif6MDqe",
        "outputId": "f8014241-e787-4d39-a61c-e41485cb6676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First embedding (not cached):\n",
            "Time taken: 0.4514 seconds\n",
            "\n",
            "Second embedding (should be cached):\n",
            "Time taken: 0.1736 seconds\n",
            "\n",
            "Speedup: 61.55%\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "# Set up the embeddings and cache\n",
        "core_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings, store, namespace=core_embeddings.model\n",
        ")\n",
        "\n",
        "# Test text\n",
        "test_text = \"This is a sample text to test cache-backed embeddings.\"\n",
        "\n",
        "# Function to measure embedding time\n",
        "def time_embedding(embedder, text):\n",
        "    start_time = time.time()\n",
        "    _ = embedder.embed_query(text)\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# First embedding (should take longer as it's not cached)\n",
        "print(\"First embedding (not cached):\")\n",
        "first_time = time_embedding(cached_embedder, test_text)\n",
        "print(f\"Time taken: {first_time:.4f} seconds\")\n",
        "\n",
        "# Second embedding (should be faster due to cache)\n",
        "print(\"\\nSecond embedding (should be cached):\")\n",
        "second_time = time_embedding(cached_embedder, test_text)\n",
        "print(f\"Time taken: {second_time:.4f} seconds\")\n",
        "\n",
        "# Calculate and print the speedup\n",
        "speedup = (first_time - second_time) / first_time * 100\n",
        "print(f\"\\nSpeedup: {speedup:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH0i-YovL8kZ"
      },
      "source": [
        "### Augmentation\n",
        "\n",
        "We'll create the classic RAG Prompt and create our `ChatPromptTemplates` as per usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WchaoMEx9j69"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_system_prompt_template = \"\"\"\\\n",
        "You are a helpful assistant that uses the provided context to answer questions. Never reference this prompt, or the existance of context.\n",
        "\"\"\"\n",
        "\n",
        "rag_message_list = [\n",
        "    {\"role\" : \"system\", \"content\" : rag_system_prompt_template},\n",
        "]\n",
        "\n",
        "rag_user_prompt_template = \"\"\"\\\n",
        "Question:\n",
        "{question}\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", rag_system_prompt_template),\n",
        "    (\"human\", rag_user_prompt_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQKnByVWMpiK"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Like usual, we'll set-up a `ChatOpenAI` model - and we'll use the fan favourite `gpt-4o-mini` for today.\n",
        "\n",
        "However, we'll also implement...a PROMPT CACHE!\n",
        "\n",
        "In essence, this works in a very similar way to the embedding cache - if we've seen this prompt before, we just use the stored response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fOXKkaY7ABab"
      },
      "outputs": [],
      "source": [
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhv8IqZoM9cY"
      },
      "source": [
        "Setting up the cache can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "thqam26gAyzN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxEovcEM_oA"
      },
      "source": [
        "##### ❓ Question #2:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "Limitations:\n",
        "1. Lack of context sensitivity: Cached responses may not account for subtle changes in context or user intent.\n",
        "2. Potential for outdated information: If the LLM is updated or fine-tuned, cached responses may become obsolete.\n",
        "3. Storage requirements: Storing a large number of prompt-response pairs can be memory-intensive.\n",
        "4. Reduced adaptability: Relying heavily on cached responses may limit the system's ability to provide novel or adaptive answers.\n",
        "\n",
        "Most useful:\n",
        "1. For frequently asked questions or common queries with stable answers.\n",
        "2. In applications requiring quick response times, such as customer service chatbots.\n",
        "3. To reduce costs associated with repeated API calls to language models.\n",
        "4. For providing consistent answers to standard queries across multiple users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iCMjVYKNEeV"
      },
      "source": [
        "##### 🏗️ Activity #2:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT5GfmsHNFqP",
        "outputId": "4b994cff-f508-4de7-da36-b2b56878c626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First response (not cached):\n",
            "Time taken: 0.7817 seconds\n",
            "\n",
            "Second response (should be cached):\n",
            "Time taken: 0.0006 seconds\n",
            "\n",
            "Speedup: 99.92%\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.caches import InMemoryCache\n",
        "from langchain_core.globals import set_llm_cache\n",
        "\n",
        "# Set up the LLM and cache\n",
        "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# Test prompt\n",
        "test_prompt = \"What is the capital of France?\"\n",
        "\n",
        "# Function to measure response time\n",
        "def time_llm_response(model, prompt):\n",
        "    start_time = time.time()\n",
        "    _ = model.invoke(prompt)\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# First response (should take longer as it's not cached)\n",
        "print(\"First response (not cached):\")\n",
        "first_time = time_llm_response(chat_model, test_prompt)\n",
        "print(f\"Time taken: {first_time:.4f} seconds\")\n",
        "\n",
        "# Second response (should be faster due to cache)\n",
        "print(\"\\nSecond response (should be cached):\")\n",
        "second_time = time_llm_response(chat_model, test_prompt)\n",
        "print(f\"Time taken: {second_time:.4f} seconds\")\n",
        "\n",
        "# Calculate and print the speedup\n",
        "speedup = (first_time - second_time) / first_time * 100\n",
        "print(f\"\\nSpeedup: {speedup:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyPnNWb9NH7W"
      },
      "source": [
        "## Task 3: RAG LCEL Chain\n",
        "\n",
        "We'll also set-up our typical RAG chain using LCEL.\n",
        "\n",
        "However, this time: We'll specifically call out that the `context` and `question` halves of the first \"link\" in the chain are executed *in parallel* by default!\n",
        "\n",
        "Thanks, LCEL!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3JNvSsx_CEtI"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | chat_prompt | chat_model\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx--wVctNdGa"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43uQegbnDQKP",
        "outputId": "7f0b9ab2-3dfd-461b-90a1-a159a0975edc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='1. The document is a Use and Care Manual for the Bosch Dishwasher model SPE68C75UC.\\n2. The author of the document is BSH Hausgeräte GmbH based in Germany.\\n3. The document consists of 60 pages in total.\\n4. It provides information on troubleshooting various issues related to the dishwasher.\\n5. The manual includes details on transportation, storage, and disposal of the appliance.\\n6. It covers topics such as removing the appliance, vacation and storage guidelines, and transporting the appliance.\\n7. The document also addresses the disposal of old appliances.\\n8. Customer service information is provided in the manual.\\n9. It includes details on the model number, production number, and consecutive numbering of the appliance.\\n10. The AquaStop® Plus Pledge is mentioned in the manual.\\n11. Technical specifications of the dishwasher are outlined in the document.\\n12. The manual includes information on Free and Open Source Software.\\n13. Details about the limited product warranty are provided.\\n14. The warranty coverage, duration, and exclusions are explained in the manual.\\n15. The document mentions an extended warranty option.\\n16. It explains the repair or replacement process as the exclusive remedy under warranty.\\n17. Out of warranty product guidelines are included in the manual.\\n18. The document discusses the colored coatings inside the dishwasher and on dishware.\\n19. It explains that films may form due to substances in vegetables or tap water.\\n20. Instructions for cleaning the appliance to remove coatings are provided.\\n21. The document mentions the discoloration of plastic parts inside the dishwasher.\\n22. It emphasizes the importance of following safety instructions.\\n23. The manual provides guidance to reduce the risk of fire, electrical shock, or injury.\\n24. Installation instructions are included in the document.\\n25. It advises reading and understanding all instructions before using the appliance.\\n26. Keeping the manual and product information in a safe place is recommended.\\n27. The document warns against connecting a damaged appliance.\\n28. Important safety instructions are highlighted in the manual.\\n29. It instructs users to read and save the provided instructions.\\n30. The manual includes information on the formation of films due to metal components.\\n31. It suggests cleaning the appliance to remove coatings caused by metal components.\\n32. Harmlessness to health is assured for the coatings on dishware.\\n33. The document provides guidance on mechanical cleaning for removing coatings.\\n34. It mentions the possibility of not completely removing coatings.\\n35. Discoloration of plastic parts over the dishwasher\\'s life is explained in the manual.\\n36. The document covers a wide range of topics related to using and caring for the dishwasher.\\n37. It serves as a comprehensive guide for users of the Bosch Dishwasher model SPE68C75UC.\\n38. The manual is structured with numbered sections for easy reference.\\n39. It offers detailed information in a systematic manner.\\n40. The document is authored by SCHEMA ST4 and produced using the ST4 PDF Engine.\\n41. It includes creation and modification dates in the metadata.\\n42. The manual is in PDF format.\\n43. The title of the document is \"Use and Care Manual SPE68C75UC | Bosch.\"\\n44. It emphasizes the importance of proper care and maintenance of the dishwasher.\\n45. The manual aims to help users troubleshoot common issues efficiently.\\n46. It provides contact information for customer service inquiries.\\n47. The document is user-friendly and easy to navigate.\\n48. It contains specific instructions tailored to the model SPE68C75UC.\\n49. The manual is designed to enhance the user experience and ensure optimal performance of the dishwasher.\\n50. Overall, the document serves as a valuable resource for users to maximize the functionality and longevity of their Bosch Dishwasher.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 752, 'prompt_tokens': 1602, 'total_tokens': 2354, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e7547497-eebd-4267-82b0-b2ecefd8596d-0', usage_metadata={'input_tokens': 1602, 'output_tokens': 752, 'total_tokens': 2354})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"Write 50 things about this document!\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tYAvHrJNecy"
      },
      "source": [
        "##### 🏗️ Activity #3:\n",
        "\n",
        "Show, through LangSmith, the different between a trace that is leveraging cache-backed embeddings and LLM calls - and one that isn't.\n",
        "\n",
        "Post screenshots in the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First run (not cached):\n",
            "Time taken: 0.3945 seconds\n",
            "\n",
            "Second run (should be cached):\n",
            "Time taken: 0.2198 seconds\n",
            "\n",
            "Speedup: 44.29%\n",
            "\n",
            "Results comparison:\n",
            "First run result:\n",
            "content='1. The document is a Use and Care Manual for the Bosch Dishwasher model SPE68C75UC.\\n2. The author of the document is BSH Hausgeräte GmbH based in Germany.\\n3. The document consists of 60 pages in total.\\n4. It provides information on troubleshooting various issues related to the dishwasher.\\n5. The manual includes details on transportation, storage, and disposal of the appliance.\\n6. It covers topics such as removing the appliance, vacation and storage guidelines, and transporting the appliance.\\n7. The document also addresses the disposal of old appliances.\\n8. Customer service information is provided in the manual.\\n9. It includes details on the model number, production number, and consecutive numbering of the appliance.\\n10. The AquaStop® Plus Pledge is mentioned in the manual.\\n11. Technical specifications of the dishwasher are outlined in the document.\\n12. The manual includes information on Free and Open Source Software.\\n13. Details about the limited product warranty are provided.\\n14. The warranty coverage, duration, and exclusions are explained in the manual.\\n15. The document mentions an extended warranty option.\\n16. It explains the repair or replacement process as the exclusive remedy under warranty.\\n17. Out of warranty product guidelines are included in the manual.\\n18. The document discusses the colored coatings inside the dishwasher and on dishware.\\n19. It explains that films may form due to substances in vegetables or tap water.\\n20. Instructions for cleaning the appliance to remove coatings are provided.\\n21. The document mentions the discoloration of plastic parts inside the dishwasher.\\n22. It emphasizes the importance of following safety instructions.\\n23. The manual provides guidance to reduce the risk of fire, electrical shock, or injury.\\n24. Installation instructions are included in the document.\\n25. It advises reading and understanding all instructions before using the appliance.\\n26. Keeping the manual and product information in a safe place is recommended.\\n27. The document warns against connecting a damaged appliance.\\n28. Important safety instructions are highlighted in the manual.\\n29. It instructs users to read and save the provided instructions.\\n30. The manual includes information on the formation of films due to metal components.\\n31. It suggests cleaning the appliance to remove coatings caused by metal components.\\n32. Harmlessness to health is assured for the coatings on dishware.\\n33. The document provides guidance on mechanical cleaning for removing coatings.\\n34. It mentions the possibility of not completely removing coatings.\\n35. Discoloration of plastic parts over the dishwasher\\'s life is explained in the manual.\\n36. The document covers a wide range of topics related to using and caring for the dishwasher.\\n37. It serves as a comprehensive guide for users of the Bosch Dishwasher model SPE68C75UC.\\n38. The manual is structured with numbered sections for easy reference.\\n39. It offers detailed information in a systematic manner.\\n40. The document is authored by SCHEMA ST4 and produced using the ST4 PDF Engine.\\n41. It includes creation and modification dates in the metadata.\\n42. The manual is in PDF format.\\n43. The title of the document is \"Use and Care Manual SPE68C75UC | Bosch.\"\\n44. It emphasizes the importance of proper care and maintenance of the dishwasher.\\n45. The manual aims to help users troubleshoot common issues efficiently.\\n46. It provides contact information for customer service inquiries.\\n47. The document is user-friendly and easy to navigate.\\n48. It contains specific instructions tailored to the model SPE68C75UC.\\n49. The manual is designed to enhance the user experience and ensure optimal performance of the dishwasher.\\n50. Overall, the document serves as a valuable resource for users to maximize the functionality and longevity of their Bosch Dishwasher.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 752, 'prompt_tokens': 1602, 'total_tokens': 2354, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-e7547497-eebd-4267-82b0-b2ecefd8596d-0' usage_metadata={'input_tokens': 1602, 'output_tokens': 752, 'total_tokens': 2354}\n",
            "\n",
            "Second run result:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set up the cache\n",
        "cache = InMemoryCache()\n",
        "\n",
        "# Add cache to the chain\n",
        "cached_chain = retrieval_augmented_qa_chain.with_config(configurable={\"cache\": cache})\n",
        "\n",
        "# Function to measure response time\n",
        "def time_chain_response(chain, input_data):\n",
        "    start_time = time.time()\n",
        "    result = chain.invoke(input_data)\n",
        "    end_time = time.time()\n",
        "    return result, end_time - start_time\n",
        "\n",
        "# Test input\n",
        "test_input = {\"question\": \"Write 50 things about this document!\"}\n",
        "\n",
        "# First run (not cached)\n",
        "print(\"First run (not cached):\")\n",
        "first_result, first_time = time_chain_response(cached_chain, test_input)\n",
        "print(f\"Time taken: {first_time:.4f} seconds\")\n",
        "\n",
        "# Second run (should be cached)\n",
        "print(\"\\nSecond run (should be cached):\")\n",
        "second_result, second_time = time_chain_response(cached_chain, test_input)\n",
        "print(f\"Time taken: {second_time:.4f} seconds\")\n",
        "\n",
        "# Calculate and print the speedup\n",
        "speedup = (first_time - second_time) / first_time * 100\n",
        "print(f\"\\nSpeedup: {speedup:.2f}%\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nResults comparison:\")\n",
        "print(\"First run result:\")\n",
        "print(first_result)\n",
        "print(\"\\nSecond run result:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing dataset: qa_eval_dataset\n",
            "Created new project: QA_Eval_20241003_165338_8336df\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Warning: create_run returned None for input: {'question': 'What are the key topics discussed in this document?'}\n",
            "Warning: create_run returned None for input: {'question': 'Summarize the main points of the document.'}\n",
            "Warning: create_run returned None for input: {'question': 'Write 50 things about this document!'}\n",
            "Evaluation completed.\n",
            "Project Name: QA_Eval_20241003_165338_8336df\n",
            "Dataset Name: qa_eval_dataset\n",
            "Number of examples processed: 45\n",
            "\n",
            "For detailed results, check the LangSmith UI.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from langchain_core.caches import InMemoryCache\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langsmith import Client\n",
        "from langchain.smith import RunEvalConfig\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.evaluation import CriteriaEvalChain\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up the cache\n",
        "cache = InMemoryCache()\n",
        "\n",
        "# Add cache to the chain\n",
        "cached_chain = retrieval_augmented_qa_chain.with_config(configurable={\"cache\": cache})\n",
        "\n",
        "# Set up LangSmith client\n",
        "client = Client()\n",
        "\n",
        "# Create a small dataset for evaluation\n",
        "eval_questions = [\n",
        "    {\"question\": \"Write 50 things about this document!\"},\n",
        "    {\"question\": \"Summarize the main points of the document.\"},\n",
        "    {\"question\": \"What are the key topics discussed in this document?\"}\n",
        "]\n",
        "\n",
        "dataset_name = \"qa_eval_dataset\"\n",
        "\n",
        "# Check if the dataset exists\n",
        "try:\n",
        "    existing_dataset = client.read_dataset(dataset_name=dataset_name)\n",
        "    print(f\"Using existing dataset: {dataset_name}\")\n",
        "except Exception:\n",
        "    # If the dataset doesn't exist, create it\n",
        "    try:\n",
        "        client.create_dataset(dataset_name, description=\"QA evaluation dataset\")\n",
        "        print(f\"Created new dataset: {dataset_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "# Add examples to the dataset\n",
        "for item in eval_questions:\n",
        "    try:\n",
        "        client.create_example(inputs=item, dataset_name=dataset_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding example to dataset: {e}\")\n",
        "\n",
        "# Create a ChatOpenAI instance for evaluation\n",
        "eval_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define custom criteria\n",
        "criteria = {\n",
        "    \"relevance\": \"The response should be highly relevant to the question asked.\",\n",
        "    \"completeness\": \"The response should fully address all aspects of the question.\",\n",
        "    \"accuracy\": \"The information provided in the response should be accurate and factual.\",\n",
        "    \"clarity\": \"The response should be clear, well-structured, and easy to understand.\"\n",
        "}\n",
        "\n",
        "# Create a CriteriaEvalChain\n",
        "criteria_eval = CriteriaEvalChain.from_llm(\n",
        "    llm=eval_llm,\n",
        "    criteria=criteria\n",
        ")\n",
        "\n",
        "# Define evaluation config\n",
        "eval_config = RunEvalConfig(\n",
        "    evaluators=[criteria_eval],\n",
        "    custom_evaluators=[],\n",
        ")\n",
        "\n",
        "# Create a unique project name\n",
        "project_name = f\"QA_Eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}\"\n",
        "\n",
        "# Create a new project\n",
        "try:\n",
        "    project = client.create_project(project_name)\n",
        "    print(f\"Created new project: {project_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating project: {e}\")\n",
        "    raise\n",
        "\n",
        "# Run evaluation on the dataset\n",
        "try:\n",
        "    # Get the examples from the dataset\n",
        "    examples = list(client.list_examples(dataset_name=dataset_name))\n",
        "    \n",
        "    for example in examples:\n",
        "        # Start a run\n",
        "        run = client.create_run(\n",
        "            project_name=project_name,\n",
        "            name=\"QA Evaluation\",\n",
        "            run_type=\"chain\",\n",
        "            inputs=example.inputs\n",
        "        )\n",
        "        \n",
        "        if run is None:\n",
        "            print(f\"Warning: create_run returned None for input: {example.inputs}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"Created run with ID: {run.id}\")\n",
        "        \n",
        "        # Manually execute the chain\n",
        "        try:\n",
        "            result = cached_chain.invoke(example.inputs)\n",
        "            \n",
        "            # Update the run with the result\n",
        "            client.update_run(\n",
        "                run.id,\n",
        "                outputs=result,\n",
        "                end_time=datetime.utcnow(),\n",
        "                error=None,\n",
        "            )\n",
        "            \n",
        "            print(f\"Updated run {run.id} with result\")\n",
        "            \n",
        "            # Run evaluators\n",
        "            for evaluator in eval_config.evaluators:\n",
        "                eval_result = evaluator.evaluate_strings(\n",
        "                    prediction=result['text'] if isinstance(result, dict) else str(result),\n",
        "                    input=example.inputs[\"question\"],\n",
        "                )\n",
        "                client.create_feedback(\n",
        "                    run.id,\n",
        "                    evaluator.__class__.__name__,\n",
        "                    score=eval_result.get(\"score\"),\n",
        "                    comment=eval_result.get(\"reasoning\"),\n",
        "                )\n",
        "            \n",
        "            print(f\"Added feedback for run {run.id}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing example: {e}\")\n",
        "            client.update_run(\n",
        "                run.id,\n",
        "                error=str(e),\n",
        "                end_time=datetime.utcnow(),\n",
        "            )\n",
        "    \n",
        "    print(\"Evaluation completed.\")\n",
        "    print(f\"Project Name: {project_name}\")\n",
        "    print(f\"Dataset Name: {dataset_name}\")\n",
        "    print(f\"Number of examples processed: {len(examples)}\")\n",
        "    print(\"\\nFor detailed results, check the LangSmith UI.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error running evaluation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
